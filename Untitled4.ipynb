{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "def f10_dnn(X_train, Y_train, nn_num_folds=10):\n",
    "    \n",
    "    folds = KFold(n_splits=nn_num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for n_fold, (nn_trn_idx, nn_val_idx) in enumerate(folds.split(X_train)):\n",
    "        nn_trn_x, nn_trn_y = X_train.iloc[nn_trn_idx,:], Y_train[nn_trn_idx]\n",
    "        nn_val_x, nn_val_y = X_train.iloc[nn_val_idx,:], Y_train[nn_val_idx]\n",
    "\n",
    "        print( 'Setting up neural network...' )\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = 718))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        nn.add(Dropout(.3))\n",
    "        nn.add(Dense(units = 12, kernel_initializer = 'normal'))\n",
    "        nn.add(PReLU())\n",
    "        nn.add(BatchNormalization())\n",
    "        nn.add(Dropout(.3))\n",
    "        nn.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "        print( 'Fitting neural network...' )\n",
    "        nn.fit(nn_trn_x, nn_trn_y, validation_data = (nn_val_x, nn_val_y), epochs=10, verbose=2,\n",
    "              callbacks=[roc_callback(training_data=(nn_trn_x, nn_trn_y),validation_data=(nn_val_x, nn_val_y))])\n",
    "        \n",
    "        #print( 'Predicting...' )\n",
    "        #sub_preds += nn.predict(X_test).flatten().clip(0,1) / folds.n_splits\n",
    "    \n",
    "        gc.collect()\n",
    "        \n",
    "        return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n",
    "    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "\n",
    "    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n",
    "    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n",
    "    df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n",
    "    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n",
    "    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n",
    "    df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    dropcolum=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\n",
    "    'FLAG_DOCUMENT_5','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7',\n",
    "    'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', \n",
    "    'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n",
    "    'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n",
    "    'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19',\n",
    "    'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n",
    "    df= df.drop(dropcolum,axis=1)\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': [ 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': [ 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': [ 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': [ 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': [ 'max', 'mean'],\n",
    "        'AMT_APPLICATION': [ 'max','mean'],\n",
    "        'AMT_CREDIT': [ 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': [ 'max', 'mean'],\n",
    "        'AMT_DOWN_PAYMENT': [ 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': [ 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': [ 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': [ 'max', 'mean'],\n",
    "        'DAYS_DECISION': [ 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum','min','std' ],\n",
    "        'DBD': ['max', 'mean', 'sum','min','std'],\n",
    "        'PAYMENT_PERC': [ 'max','mean',  'var','min','std'],\n",
    "        'PAYMENT_DIFF': [ 'max','mean', 'var','min','std'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum','min','std'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum','std'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum','std']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('/Users/fairy/Documents/data science/kaggle_summer18/code/input/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg([ 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        #else: df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(to_load=False):\n",
    "    \n",
    "    if not to_load:\n",
    "        \n",
    "        df = data_builder()\n",
    "        feats = [f for f in df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "        \n",
    "        # Split to train and test:\n",
    "        y = df['TARGET']\n",
    "        X = df[feats]\n",
    "        X = X.fillna(X.mean()).clip(-1e11,1e11)\n",
    "\n",
    "        print(\"X shape: \", X.shape, \"    y shape:\", y.shape)\n",
    "        print(\"\\nPreparing data...\")\n",
    "\n",
    "        training = y.notnull()\n",
    "        testing = y.isnull()\n",
    "        \n",
    "        X_train = X.loc[training,:]\n",
    "        X_test = X.loc[testing,:]\n",
    "        y_train = y.loc[training]\n",
    "        \n",
    "        # Scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(X)\n",
    "        X_train.loc[:, X_train.columns] = scaler.transform(X_train[X_train.columns])\n",
    "        X_test.loc[:, X_test.columns] = scaler.transform(X_test[X_test.columns])\n",
    "        \n",
    "        print(X_train.shape, X_test.shape, y_train.shape)\n",
    "        df.to_pickle('df_low_mem.pkl.gz')\n",
    "        \n",
    "        del df, X, y, training, testing\n",
    "        gc.collect()\n",
    "    \n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_builder():\n",
    "    \n",
    "    num_rows = None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "        \n",
    "    df.set_index('SK_ID_CURR', inplace=True, drop=False)\n",
    "    df = df.drop(labels='index', axis=1)\n",
    "    df = reduce_mem_usage(df)    \n",
    "    df.to_pickle('df_low_mem.pkl.gz')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 95)\n",
      "Process bureau and bureau_balance - done in 58s\n",
      "Previous applications df shape: (338857, 219)\n",
      "Process previous_applications - done in 60s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in 27s\n",
      "Installments payments df shape: (339587, 36)\n",
      "Process installments payments - done in 49s\n",
      "Credit card balance df shape: (103558, 113)\n",
      "Process credit card balance - done in 28s\n",
      "Memory usage of dataframe is 1643.36 MB\n",
      "Memory usage after optimization is: 595.58 MB\n",
      "Decreased by 63.8%\n",
      "X shape:  (356251, 718)     y shape: (356251,)\n",
      "\n",
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairy/anaconda/envs/data_science_initiative_env/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/fairy/anaconda/envs/data_science_initiative_env/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/fairy/anaconda/envs/data_science_initiative_env/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307507, 718) (48744, 718) (307507,)\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed:\n",
    "seed_val = 42\n",
    "\n",
    "# Load data:\n",
    "train_x, test_x, train_y = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holdout\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_hold, y_train, y_hold = train_test_split(train_x, train_y, test_size=0.1, random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import sklearn.linear_model\n",
    "\n",
    "def ridge(trn_x, trn_y):\n",
    "    clf = Ridge(alpha=20, \n",
    "                copy_X=True, \n",
    "                fit_intercept=True, \n",
    "                solver='auto',max_iter=10000,\n",
    "                normalize=False, \n",
    "                random_state=0,  \n",
    "                tol=0.0025)\n",
    "    clf.fit(trn_x, trn_y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = ['ridge','f10_dnn']\n",
    "pred_cols = ['ridge','f10_dnn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_regression_stacker(train_x, train_y, test_x,\n",
    "                           estimators, \n",
    "                           pred_cols, \n",
    "                           train_eval_metric, \n",
    "                           compare_eval_metric,\n",
    "                           n_folds = 3,\n",
    "                           holdout_x=False,\n",
    "                           debug = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Original script:\n",
    "        Jovan Sardinha\n",
    "        https://medium.com/weightsandbiases/an-introduction-to-model-ensembling-63effc2ca4b3\n",
    "        \n",
    "    Args:\n",
    "        train_x, train_y, test_x (DataFrame).\n",
    "        n_folds (int): The number of folds for crossvalidation.\n",
    "        esdtimators (list): The list of estimator functions.\n",
    "        pred_cols (list): The estimator related names of prediction columns.\n",
    "        train_eval_metric (class): Fucntion for the train eval metric.\n",
    "        compare_eval_metric (class): Fucntion for the crossvalidation eval metric.\n",
    "        holdout_x (DataFrame): Holdout dataframe if you intend to stack/blend using holdout.\n",
    "            Returns:\n",
    "        train_blend, test_blend, model\n",
    "    \"\"\"\n",
    "    \n",
    "    if debug == True:\n",
    "        train_x = train_x.sample(n=1000, random_state=seed_val)\n",
    "        train_y = train_y.sample(n=1000, random_state=seed_val)\n",
    "        \n",
    "    # Start timer:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # List to save models:\n",
    "    model_list = []\n",
    "    \n",
    "    # Initializing blending data frames:\n",
    "    with_holdout = isinstance(holdout_x, pd.DataFrame)\n",
    "    if with_holdout: holdout_blend = pd.DataFrame(holdout_x.index)\n",
    "    \n",
    "    train_blend = pd.DataFrame(train_x.index)\n",
    "    val_blend = pd.DataFrame(train_x.index)\n",
    "    test_blend = pd.DataFrame(test_x.index)\n",
    "\n",
    "    # Arrays to hold estimators' predictions:\n",
    "    test_len = test_x.shape[0]\n",
    "    train_len = train_x.shape[0]\n",
    "\n",
    "    dataset_blend_train = np.zeros((train_len, len(estimators))) # Mean train prediction holder\n",
    "    dataset_blend_val = np.zeros((train_len, len(estimators))) # Validfation prediction holder                   \n",
    "    dataset_blend_test = np.zeros((test_len, len(estimators))) # Mean test prediction holder\n",
    "    if with_holdout: dataset_blend_holdout = np.zeros((holdout_x.shape[0], len(estimators))) # Same for holdout\n",
    "        \n",
    "    # Note: StratifiedKFold splits into roughly 66% train 33% test  \n",
    "    folds = StratifiedShuffleSplit(n_splits= n_folds, random_state=seed_val,\n",
    "                                  test_size = 1/n_folds, train_size = 1-(1/n_folds))\n",
    "        \n",
    "    # For every estimator:\n",
    "    for j, estimator in enumerate(estimators):\n",
    "        \n",
    "        # Array to hold folds number of predictions on test:\n",
    "        dataset_blend_train_j = np.zeros((train_len, n_folds))\n",
    "        dataset_blend_test_j = np.zeros((test_len, n_folds))\n",
    "        if with_holdout: dataset_blend_holdout_j = np.zeros((holdout_x.shape[0], n_folds))\n",
    "        \n",
    "        # For every fold:\n",
    "        for i, (train, test) in enumerate(folds.split(train_x, train_y)):\n",
    "            trn_x = train_x.iloc[train, :] \n",
    "            trn_y = train_y.iloc[train].values.ravel()\n",
    "            val_x = train_x.iloc[test, :] \n",
    "            val_y = train_y.iloc[test].values.ravel()\n",
    "            \n",
    "            # Estimators conditional training:\n",
    "            if estimator == 'lgb':\n",
    "                model = kfold_lightgbm(trn_x, trn_y)\n",
    "                pred_val = model.predict(val_x)\n",
    "                pred_test = model.predict(test_x)\n",
    "                pred_train = model.predict(train_x)\n",
    "                if with_holdout:\n",
    "                    pred_holdout = model.predict(holdout_x)                \n",
    "            elif estimator == 'xgb':\n",
    "                model = kfold_xgb(trn_x, trn_y)\n",
    "                pred_val = xgb_predict(val_x, model)\n",
    "                pred_test = xgb_predict(test_x, model)\n",
    "                pred_train = xgb_predict(train_x, model)\n",
    "                if with_holdout:\n",
    "                    pred_holdout = xgb_predict(holdout_x, model)\n",
    "            elif estimator == 'f10_dnn':\n",
    "                model = f10_dnn(trn_x, trn_y)\n",
    "                pred_val = model.predict(val_x).ravel()\n",
    "                pred_test = model.predict(test_x).ravel()\n",
    "                pred_train = model.predict(train_x).ravel()\n",
    "                if with_holdout:\n",
    "                    pred_holdout = model.predict(holdout_x).ravel()\n",
    "                #print(pred_val.shape, pred_test.shape, pred_train.shape)             \n",
    "            elif estimator == 'ridge':\n",
    "                model = ridge(trn_x, trn_y)\n",
    "                pred_val = model.predict(val_x)\n",
    "                pred_test = model.predict(test_x)\n",
    "                pred_train = model.predict(train_x)\n",
    "                if with_holdout:\n",
    "                    pred_holdout = model.predict(holdout_x)                         \n",
    "            else:\n",
    "                model = kfold_cat(trn_x, trn_y)\n",
    "                pred_val = model.predict_proba(val_x)[:,1]\n",
    "                pred_test = model.predict_proba(test_x)[:,1]\n",
    "                pred_train = model.predict_proba(train_x)[:,1]\n",
    "                if with_holdout:\n",
    "                    pred_holdout = model.predict_proba(holdout_x)[:,1]         \n",
    "            \n",
    "            dataset_blend_val[test, j] = pred_val\n",
    "            dataset_blend_test_j[:, i] = pred_test\n",
    "            dataset_blend_train_j[:, i] = pred_train\n",
    "            if with_holdout: \n",
    "                dataset_blend_holdout_j[:, i] = pred_holdout\n",
    "            \n",
    "            print('fold:', i+1, '/', n_folds,\n",
    "                  '; estimator:',  j+1, '/', len(estimators),\n",
    "                  ' -> oof cv score:', compare_eval_metric(val_y, pred_val))\n",
    "\n",
    "            del trn_x, trn_y, val_x, val_y\n",
    "            gc.collect()\n",
    "    \n",
    "        # Save curent estimator's mean prediction for test, train and holdout:\n",
    "        dataset_blend_test[:, j] = np.mean(dataset_blend_test_j, axis=1)\n",
    "        dataset_blend_train[:, j] = np.mean(dataset_blend_train_j, axis=1)\n",
    "        if with_holdout: dataset_blend_holdout[:, j] = np.mean(dataset_blend_holdout_j, axis=1)\n",
    "        \n",
    "        model_list += [model]\n",
    "        \n",
    "    #print('--- comparing models ---')\n",
    "    for i in range(dataset_blend_val.shape[1]):\n",
    "        print('model', i+1, ':', compare_eval_metric(train_y, dataset_blend_val[:,i]))\n",
    "        \n",
    "    for i, j in enumerate(estimators):\n",
    "        val_blend[pred_cols[i]] = dataset_blend_val[:,i]\n",
    "        test_blend[pred_cols[i]] = dataset_blend_test[:,i]\n",
    "        train_blend[pred_cols[i]] = dataset_blend_train[:,i]\n",
    "        if with_holdout: \n",
    "            holdout_blend[pred_cols[i]] = dataset_blend_holdout[:,i]\n",
    "        else:\n",
    "            holdout_blend = False\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Total Time usage: \" + str(int(round(end_time - start_time))))\n",
    "    return train_blend, val_blend, test_blend, holdout_blend, model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 / 2 ; estimator: 1 / 2  -> oof cv score: 0.6357416986484231\n",
      "fold: 2 / 2 ; estimator: 1 / 2  -> oof cv score: 0.6321263696534847\n",
      "Setting up neural network...\n",
      "Fitting neural network...\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.6937 - val_loss: 0.6802\n",
      "roc-auc: 0.5683 - roc-auc_val: 0.4457                                                                                                    \n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6814 - val_loss: 0.6696\n",
      "roc-auc: 0.6821 - roc-auc_val: 0.4565                                                                                                    \n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6778 - val_loss: 0.6669\n",
      "roc-auc: 0.7248 - roc-auc_val: 0.4076                                                                                                    \n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6663 - val_loss: 0.6690\n",
      "roc-auc: 0.6883 - roc-auc_val: 0.6413                                                                                                    \n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6560 - val_loss: 0.6695\n",
      "roc-auc: 0.6952 - roc-auc_val: 0.4565                                                                                                    \n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6511 - val_loss: 0.6564\n",
      "roc-auc: 0.7009 - roc-auc_val: 0.5652                                                                                                    \n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6406 - val_loss: 0.6251\n",
      "roc-auc: 0.7338 - roc-auc_val: 0.7337                                                                                                    \n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6278 - val_loss: 0.5843\n",
      "roc-auc: 0.7167 - roc-auc_val: 0.7446                                                                                                    \n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6131 - val_loss: 0.5649\n",
      "roc-auc: 0.7937 - roc-auc_val: 0.6739                                                                                                    \n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6077 - val_loss: 0.5302\n",
      "roc-auc: 0.7611 - roc-auc_val: 0.6739                                                                                                    \n",
      "fold: 1 / 2 ; estimator: 2 / 2  -> oof cv score: 0.6196117692863896\n",
      "Setting up neural network...\n",
      "Fitting neural network...\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.7029 - val_loss: 0.6826\n",
      "roc-auc: 0.4883 - roc-auc_val: 0.2935                                                                                                    \n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6947 - val_loss: 0.6956\n",
      "roc-auc: 0.5324 - roc-auc_val: 0.5978                                                                                                    \n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6711 - val_loss: 0.7044\n",
      "roc-auc: 0.4943 - roc-auc_val: 0.5815                                                                                                    \n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6716 - val_loss: 0.6758\n",
      "roc-auc: 0.6033 - roc-auc_val: 0.4348                                                                                                    \n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6583 - val_loss: 0.6718\n",
      "roc-auc: 0.6467 - roc-auc_val: 0.4783                                                                                                    \n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6484 - val_loss: 0.6794\n",
      "roc-auc: 0.6757 - roc-auc_val: 0.25                                                                                                    \n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6419 - val_loss: 0.6999\n",
      "roc-auc: 0.6626 - roc-auc_val: 0.3043                                                                                                    \n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6259 - val_loss: 0.7556\n",
      "roc-auc: 0.6244 - roc-auc_val: 0.3913                                                                                                    \n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6200 - val_loss: 0.6579\n",
      "roc-auc: 0.7042 - roc-auc_val: 0.337                                                                                                    \n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6078 - val_loss: 0.5319\n",
      "roc-auc: 0.7391 - roc-auc_val: 0.337                                                                                                    \n",
      "fold: 2 / 2 ; estimator: 2 / 2  -> oof cv score: 0.5898548306357417\n",
      "model 1 : 0.5820540630735859\n",
      "model 2 : 0.5454697146671116\n",
      "Total Time usage: 21\n"
     ]
    }
   ],
   "source": [
    "n_folds = 2\n",
    "tr_blend, va_blend, tst_blend, hold_blend, m_list = oof_regression_stacker(x_train, y_train, test_x, \n",
    "                                                                           n_folds = 2, \n",
    "                                                                           estimators=estimators, \n",
    "                                                                           pred_cols = pred_cols,\n",
    "                                                                           train_eval_metric=roc_auc_score,\n",
    "                                                                           compare_eval_metric=roc_auc_score,\n",
    "                                                                           debug = True, holdout_x = x_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_blend.to_csv('tr_blend_nn.csv')\n",
    "va_blend.to_csv('va_blend_nn.csv')\n",
    "tst_blend.to_csv('tst_blend_nn.csv')\n",
    "hold_blend.to_csv('hold_blend_nn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_blend.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_blend.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146232"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_blend.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92253"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold_blend.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(m_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>ridge</th>\n",
       "      <th>f10_dnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.039733</td>\n",
       "      <td>0.373474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.127522</td>\n",
       "      <td>0.395218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>-0.023535</td>\n",
       "      <td>0.388663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.070315</td>\n",
       "      <td>0.376839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.185160</td>\n",
       "      <td>0.386301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR     ridge   f10_dnn\n",
       "0      100001  0.039733  0.373474\n",
       "1      100005  0.127522  0.395218\n",
       "2      100013 -0.023535  0.388663\n",
       "3      100028  0.070315  0.376839\n",
       "4      100038  0.185160  0.386301"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_blend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 0.75*tst_blend.f10_dnn + 0.25*tst_blend.ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_blend['TARGET'] = TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_new = tst_blend.drop(['ridge','f10_dnn'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_new.to_csv('kernel04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_new2 = pd.read_csv('submission_kernel03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET1 = 0.05*TARGET + 0.95*tst_new2.TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_new2.drop('TARGET',axis=1)\n",
    "tst_new2['TARGET'] = TARGET1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_new2.to_csv('kernel05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.063953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.085466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.060460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.086893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100042</td>\n",
       "      <td>0.061407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100057</td>\n",
       "      <td>0.059338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100065</td>\n",
       "      <td>0.060073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100066</td>\n",
       "      <td>0.058156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100067</td>\n",
       "      <td>0.075469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100074</td>\n",
       "      <td>0.065541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100090</td>\n",
       "      <td>0.077998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100091</td>\n",
       "      <td>0.090247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100092</td>\n",
       "      <td>0.071388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100106</td>\n",
       "      <td>0.074741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100107</td>\n",
       "      <td>0.091636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100109</td>\n",
       "      <td>0.062878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100117</td>\n",
       "      <td>0.062294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100128</td>\n",
       "      <td>0.067321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100141</td>\n",
       "      <td>0.060501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100150</td>\n",
       "      <td>0.064665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100168</td>\n",
       "      <td>0.058880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100169</td>\n",
       "      <td>0.067513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100170</td>\n",
       "      <td>0.091906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100171</td>\n",
       "      <td>0.063463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100172</td>\n",
       "      <td>0.084917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100184</td>\n",
       "      <td>0.073833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100187</td>\n",
       "      <td>0.069431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100212</td>\n",
       "      <td>0.062423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100222</td>\n",
       "      <td>0.063014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48714</th>\n",
       "      <td>455963</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48715</th>\n",
       "      <td>455965</td>\n",
       "      <td>0.058039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48716</th>\n",
       "      <td>456007</td>\n",
       "      <td>0.101576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48717</th>\n",
       "      <td>456008</td>\n",
       "      <td>0.065036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48718</th>\n",
       "      <td>456009</td>\n",
       "      <td>0.067304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48719</th>\n",
       "      <td>456010</td>\n",
       "      <td>0.075968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48720</th>\n",
       "      <td>456011</td>\n",
       "      <td>0.060102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48721</th>\n",
       "      <td>456013</td>\n",
       "      <td>0.110003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48722</th>\n",
       "      <td>456028</td>\n",
       "      <td>0.090504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48723</th>\n",
       "      <td>456058</td>\n",
       "      <td>0.074151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48724</th>\n",
       "      <td>456111</td>\n",
       "      <td>0.073133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48725</th>\n",
       "      <td>456114</td>\n",
       "      <td>0.073970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48726</th>\n",
       "      <td>456115</td>\n",
       "      <td>0.061291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48727</th>\n",
       "      <td>456116</td>\n",
       "      <td>0.059159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48728</th>\n",
       "      <td>456119</td>\n",
       "      <td>0.067346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48729</th>\n",
       "      <td>456120</td>\n",
       "      <td>0.086330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48730</th>\n",
       "      <td>456122</td>\n",
       "      <td>0.067286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48731</th>\n",
       "      <td>456123</td>\n",
       "      <td>0.063132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48732</th>\n",
       "      <td>456166</td>\n",
       "      <td>0.085851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48733</th>\n",
       "      <td>456167</td>\n",
       "      <td>0.060979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48734</th>\n",
       "      <td>456168</td>\n",
       "      <td>0.071646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48735</th>\n",
       "      <td>456169</td>\n",
       "      <td>0.083247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48736</th>\n",
       "      <td>456170</td>\n",
       "      <td>0.057983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48737</th>\n",
       "      <td>456189</td>\n",
       "      <td>0.074748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48738</th>\n",
       "      <td>456202</td>\n",
       "      <td>0.067458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48739</th>\n",
       "      <td>456221</td>\n",
       "      <td>0.072512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48740</th>\n",
       "      <td>456222</td>\n",
       "      <td>0.070602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48741</th>\n",
       "      <td>456223</td>\n",
       "      <td>0.051349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48742</th>\n",
       "      <td>456224</td>\n",
       "      <td>0.059266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>456250</td>\n",
       "      <td>0.106556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SK_ID_CURR    TARGET\n",
       "0          100001  0.063953\n",
       "1          100005  0.085466\n",
       "2          100013  0.060460\n",
       "3          100028  0.062500\n",
       "4          100038  0.086893\n",
       "5          100042  0.061407\n",
       "6          100057  0.059338\n",
       "7          100065  0.060073\n",
       "8          100066  0.058156\n",
       "9          100067  0.075469\n",
       "10         100074  0.065541\n",
       "11         100090  0.077998\n",
       "12         100091  0.090247\n",
       "13         100092  0.071388\n",
       "14         100106  0.074741\n",
       "15         100107  0.091636\n",
       "16         100109  0.062878\n",
       "17         100117  0.062294\n",
       "18         100128  0.067321\n",
       "19         100141  0.060501\n",
       "20         100150  0.064665\n",
       "21         100168  0.058880\n",
       "22         100169  0.067513\n",
       "23         100170  0.091906\n",
       "24         100171  0.063463\n",
       "25         100172  0.084917\n",
       "26         100184  0.073833\n",
       "27         100187  0.069431\n",
       "28         100212  0.062423\n",
       "29         100222  0.063014\n",
       "...           ...       ...\n",
       "48714      455963  0.059100\n",
       "48715      455965  0.058039\n",
       "48716      456007  0.101576\n",
       "48717      456008  0.065036\n",
       "48718      456009  0.067304\n",
       "48719      456010  0.075968\n",
       "48720      456011  0.060102\n",
       "48721      456013  0.110003\n",
       "48722      456028  0.090504\n",
       "48723      456058  0.074151\n",
       "48724      456111  0.073133\n",
       "48725      456114  0.073970\n",
       "48726      456115  0.061291\n",
       "48727      456116  0.059159\n",
       "48728      456119  0.067346\n",
       "48729      456120  0.086330\n",
       "48730      456122  0.067286\n",
       "48731      456123  0.063132\n",
       "48732      456166  0.085851\n",
       "48733      456167  0.060979\n",
       "48734      456168  0.071646\n",
       "48735      456169  0.083247\n",
       "48736      456170  0.057983\n",
       "48737      456189  0.074748\n",
       "48738      456202  0.067458\n",
       "48739      456221  0.072512\n",
       "48740      456222  0.070602\n",
       "48741      456223  0.051349\n",
       "48742      456224  0.059266\n",
       "48743      456250  0.106556\n",
       "\n",
       "[48744 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
